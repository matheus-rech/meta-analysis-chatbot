{
  "start_time": "2025-08-10T21:16:38.608773",
  "end_time": "2025-08-10T21:17:13.851974",
  "duration": 35.243201,
  "results": {
    "MCP Server Functional Tests": {
      "name": "MCP Server Functional Tests",
      "passed": 0,
      "failed": 1,
      "errors": 0,
      "skipped": 0,
      "duration": 9.209903001785278,
      "success": false,
      "output": "tinuous_outcome_mean_difference FAILED [ 22%]\ntest_mcp_server_functional.py::TestMCPServerFunctional::test_standardized_mean_difference FAILED [ 33%]\ntest_mcp_server_functional.py::TestMCPServerFunctional::test_publication_bias_with_sufficient_studies FAILED [ 44%]\ntest_mcp_server_functional.py::TestMCPServerFunctional::test_forest_plot_generation FAILED [ 55%]\ntest_mcp_server_functional.py::TestMCPServerFunctional::test_heterogeneity_investigation FAILED [ 66%]\ntest_mcp_server_functional.py::TestMCPServerFunctional::test_sensitivity_analysis FAILED [ 77%]\ntest_mcp_server_functional.py::TestMCPServerFunctional::test_cochrane_recommendations_integration FAILED [ 88%]\ntest_mcp_server_functional.py::TestMCPServerFunctional::test_report_generation FAILED [100%]\n\n=================================== FAILURES ===================================\n____________ TestMCPServerFunctional.test_clinical_trial_odds_ratio ____________\ntest_mcp_server_functional.py:89: in test_clinical_trial_odds_ratio\n    response = self.send_request(server_process, \"tools/call\", {\ntest_mcp_server_functional.py:59: in send_request\n    proc.stdin.flush()\nE   BrokenPipeError: [Errno 32] Broken pipe\n_______ TestMCPServerFunctional.test_continuous_outcome_mean_difference ________\ntest_mcp_server_functional.py:157: in test_continuous_outcome_mean_difference\n    response = self.send_request(server_process, \"tools/call\", {\ntest_mcp_server_functional.py:59: in send_request\n    proc.stdin.flush()\nE   BrokenPipeError: [Errno 32] Broken pipe\n__________ TestMCPServerFunctional.test_standardized_mean_difference ___________\ntest_mcp_server_functional.py:213: in test_standardized_mean_difference\n    response = self.send_request(server_process, \"tools/call\", {\ntest_mcp_server_functional.py:59: in send_request\n    proc.stdin.flush()\nE   BrokenPipeError: [Errno 32] Broken pipe\n____ TestMCPServerFunctional.test_publication_bias_with_sufficient_studies _____\ntest_mcp_server_functional.py:275: in test_publication_bias_with_sufficient_studies\n    response = self.send_request(server_process, \"tools/call\", {\ntest_mcp_server_functional.py:59: in send_request\n    proc.stdin.flush()\nE   BrokenPipeError: [Errno 32] Broken pipe\n_____________ TestMCPServerFunctional.test_forest_plot_generation ______________\ntest_mcp_server_functional.py:331: in test_forest_plot_generation\n    response = self.send_request(server_process, \"tools/call\", {\ntest_mcp_server_functional.py:59: in send_request\n    proc.stdin.flush()\nE   BrokenPipeError: [Errno 32] Broken pipe\n___________ TestMCPServerFunctional.test_heterogeneity_investigation ___________\ntest_mcp_server_functional.py:397: in test_heterogeneity_investigation\n    response = self.send_request(server_process, \"tools/call\", {\ntest_mcp_server_functional.py:59: in send_request\n    proc.stdin.flush()\nE   BrokenPipeError: [Errno 32] Broken pipe\n______________ TestMCPServerFunctional.test_sensitivity_analysis _______________\ntest_mcp_server_functional.py:450: in test_sensitivity_analysis\n    response = self.send_request(server_process, \"tools/call\", {\ntest_mcp_server_functional.py:59: in send_request\n    proc.stdin.flush()\nE   BrokenPipeError: [Errno 32] Broken pipe\n______ TestMCPServerFunctional.test_cochrane_recommendations_integration _______\ntest_mcp_server_functional.py:499: in test_cochrane_recommendations_integration\n    response = self.send_request(server_process, \"tools/call\", {\ntest_mcp_server_functional.py:59: in send_request\n    proc.stdin.flush()\nE   BrokenPipeError: [Errno 32] Broken pipe\n________________ TestMCPServerFunctional.test_report_generation ________________\ntest_mcp_server_functional.py:555: in test_report_generation\n    response = self.send_request(server_process, \"tools/call\", {\ntest_mcp_server_functional.py:59: in send_request\n    proc.stdin.flush()\nE   BrokenPipeError: [Errno 32] Broken pipe\n=========================== short test summary info ============================\nFAILED test_mcp_server_functional.py::TestMCPServerFunctional::test_clinical_trial_odds_ratio\nFAILED test_mcp_server_functional.py::TestMCPServerFunctional::test_continuous_outcome_mean_difference\nFAILED test_mcp_server_functional.py::TestMCPServerFunctional::test_standardized_mean_difference\nFAILED test_mcp_server_functional.py::TestMCPServerFunctional::test_publication_bias_with_sufficient_studies\nFAILED test_mcp_server_functional.py::TestMCPServerFunctional::test_forest_plot_generation\nFAILED test_mcp_server_functional.py::TestMCPServerFunctional::test_heterogeneity_investigation\nFAILED test_mcp_server_functional.py::TestMCPServerFunctional::test_sensitivity_analysis\nFAILED test_mcp_server_functional.py::TestMCPServerFunctional::test_cochrane_recommendations_integration\nFAILED test_mcp_server_functional.py::TestMCPServerFunctional::test_report_generation\n============================== 9 failed in 4.96s ===============================\nException ignored in: <_io.TextIOWrapper name=12 encoding='UTF-8'>\nBrokenPipeError: [Errno 32] Broken pipe\n"
    },
    "Gradio UI Playwright Tests": {
      "name": "Gradio UI Playwright Tests",
      "passed": 0,
      "failed": 1,
      "errors": 84,
      "skipped": 0,
      "duration": 5.2359089851379395,
      "success": false,
      "output": "ndling_invalid_data' requested an async fixture 'page', with no plugin or hook that handled it. This is usually an error, as pytest does not natively support it. This will turn into an error in pytest 9.\n  See: https://docs.pytest.org/en/stable/deprecations.html#sync-test-depending-on-async-fixture\n    warnings.warn(\n\ntest_gradio_ui_playwright.py::TestGradioMetaAnalysisUI::test_error_handling_invalid_data\n  /Users/matheusrech/.python-global/lib/python3.13/site-packages/pytest_asyncio/plugin.py:598: PytestDeprecationWarning: asyncio test 'test_error_handling_invalid_data' requested async @pytest.fixture 'page' in strict mode. You might want to use @pytest_asyncio.fixture or switch to auto mode. This will become an error in future versions of flake8-asyncio.\n    warnings.warn(\n\ntest_gradio_ui_playwright.py::TestGradioMetaAnalysisUI::test_error_handling_invalid_data\n  /Users/matheusrech/.python-global/lib/python3.13/site-packages/pytest_asyncio/plugin.py:598: PytestDeprecationWarning: asyncio test 'test_error_handling_invalid_data' requested async @pytest.fixture 'browser_context' in strict mode. You might want to use @pytest_asyncio.fixture or switch to auto mode. This will become an error in future versions of flake8-asyncio.\n    warnings.warn(\n\ntest_gradio_ui_playwright.py::TestGradioMetaAnalysisUI::test_error_handling_missing_session\n  /Users/matheusrech/.python-global/lib/python3.13/site-packages/_pytest/fixtures.py:1181: PytestRemovedIn9Warning: 'test_error_handling_missing_session' requested an async fixture 'browser_context', with no plugin or hook that handled it. This is usually an error, as pytest does not natively support it. This will turn into an error in pytest 9.\n  See: https://docs.pytest.org/en/stable/deprecations.html#sync-test-depending-on-async-fixture\n    warnings.warn(\n\ntest_gradio_ui_playwright.py::TestGradioMetaAnalysisUI::test_error_handling_missing_session\n  /Users/matheusrech/.python-global/lib/python3.13/site-packages/_pytest/fixtures.py:1181: PytestRemovedIn9Warning: 'test_error_handling_missing_session' requested an async fixture 'page', with no plugin or hook that handled it. This is usually an error, as pytest does not natively support it. This will turn into an error in pytest 9.\n  See: https://docs.pytest.org/en/stable/deprecations.html#sync-test-depending-on-async-fixture\n    warnings.warn(\n\ntest_gradio_ui_playwright.py::TestGradioMetaAnalysisUI::test_error_handling_missing_session\n  /Users/matheusrech/.python-global/lib/python3.13/site-packages/pytest_asyncio/plugin.py:598: PytestDeprecationWarning: asyncio test 'test_error_handling_missing_session' requested async @pytest.fixture 'page' in strict mode. You might want to use @pytest_asyncio.fixture or switch to auto mode. This will become an error in future versions of flake8-asyncio.\n    warnings.warn(\n\ntest_gradio_ui_playwright.py::TestGradioMetaAnalysisUI::test_error_handling_missing_session\n  /Users/matheusrech/.python-global/lib/python3.13/site-packages/pytest_asyncio/plugin.py:598: PytestDeprecationWarning: asyncio test 'test_error_handling_missing_session' requested async @pytest.fixture 'browser_context' in strict mode. You might want to use @pytest_asyncio.fixture or switch to auto mode. This will become an error in future versions of flake8-asyncio.\n    warnings.warn(\n\n-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n- Generated html report: file:///Users/matheusrech/MEGA/Meta-Repo/meta-analysis-chatbot/tests/playwright_report.html -\n=========================== short test summary info ============================\nFAILED test_gradio_ui_playwright.py::TestGradioMetaAnalysisUI::test_ui_elements_present\nFAILED test_gradio_ui_playwright.py::TestGradioMetaAnalysisUI::test_chatbot_basic_conversation\nFAILED test_gradio_ui_playwright.py::TestGradioMetaAnalysisUI::test_chatbot_meta_analysis_workflow\nFAILED test_gradio_ui_playwright.py::TestGradioMetaAnalysisUI::test_chatbot_educational_content\nFAILED test_gradio_ui_playwright.py::TestGradioMetaAnalysisUI::test_direct_tools_initialization\nFAILED test_gradio_ui_playwright.py::TestGradioMetaAnalysisUI::test_direct_tools_data_upload\nFAILED test_gradio_ui_playwright.py::TestGradioMetaAnalysisUI::test_performance_small_dataset\nFAILED test_gradio_ui_playwright.py::TestGradioMetaAnalysisUI::test_performance_medium_dataset\nFAILED test_gradio_ui_playwright.py::TestGradioMetaAnalysisUI::test_performance_large_dataset\nFAILED test_gradio_ui_playwright.py::TestGradioMetaAnalysisUI::test_heterogeneity_detection\nFAILED test_gradio_ui_playwright.py::TestGradioMetaAnalysisUI::test_forest_plot_generation\nFAILED test_gradio_ui_playwright.py::TestGradioMetaAnalysisUI::test_publication_bias_assessment\nFAILED test_gradio_ui_playwright.py::TestGradioMetaAnalysisUI::test_error_handling_invalid_data\nFAILED test_gradio_ui_playwright.py::TestGradioMetaAnalysisUI::test_error_handling_missing_session\n======================= 14 failed, 56 warnings in 2.16s ========================\n"
    },
    "MCP Client Integration Tests": {
      "name": "MCP Client Integration Tests",
      "passed": 0,
      "failed": 0,
      "errors": 0,
      "skipped": 0,
      "duration": 0.09284281730651855,
      "success": false,
      "output": "/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/Resources/Python.app/Contents/MacOS/Python: can't open file '/Users/matheusrech/MEGA/Meta-Repo/meta-analysis-chatbot/tests/test_mcp_clients.py': [Errno 2] No such file or directory\n"
    }
  },
  "summary": {
    "total_passed": 0,
    "total_failed": 2,
    "total_errors": 84,
    "total_skipped": 0,
    "success_rate": 0.0
  }
}